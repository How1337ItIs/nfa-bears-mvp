'use client';\n\nimport { useEffect, useState, useRef, useCallback } from 'react';\n\n// CENTRALIZED AUDIO REACTIVITY - Single analyzer for entire app\n// Per ChatGPT-5 recommendation: avoid multiple analyzers per route\n\ninterface AudioAnalysisData {\n  bass: number;          // 0-1, affects splat force & oil thickness\n  mids: number;          // 0-1, affects flow velocity\n  treble: number;        // 0-1, affects surface iridescence  \n  volume: number;        // 0-1, overall intensity multiplier\n  beatDetected: boolean; // beat detection for pulse effects\n  spectralData: Float32Array; // raw frequency data for advanced use\n}\n\ninterface AudioReactiveParams {\n  splatForce: number;    // Bass -> splat intensity (5-25 range)\n  thermalRate: number;   // Mids -> convection frequency (1-10 range)\n  colorPhase: number;    // Treble -> thin-film color shift (0-2π range)\n  globalIntensity: number; // Volume -> overall effect strength (0.3-1.2 range)\n}\n\n// UNIFIED AUDIO-TO-PHYSICS MAPPING (single source of truth)\n// Prevents different components from using conflicting mappings\nfunction mapAudioToPhysics(audio: AudioAnalysisData): AudioReactiveParams {\n  return {\n    splatForce: 8 + audio.bass * 15,              // 8-23 range for authentic physics\n    thermalRate: 2 + audio.mids * 6,              // 2-8 convection splats per 10s\n    colorPhase: audio.treble * Math.PI * 2,       // 0-2π for color rotation\n    globalIntensity: 0.4 + audio.volume * 0.6    // 0.4-1.0 prevents invisible effects\n  };\n}\n\nexport function useAudioReactiveEngine() {\n  const [audioData, setAudioData] = useState<AudioAnalysisData>({\n    bass: 0.3,\n    mids: 0.3, \n    treble: 0.3,\n    volume: 0.5,\n    beatDetected: false,\n    spectralData: new Float32Array(128)\n  });\n  \n  const [physicsParams, setPhysicsParams] = useState<AudioReactiveParams>(\n    mapAudioToPhysics(audioData)\n  );\n  \n  const analyserRef = useRef<AnalyserNode>();\n  const beatDetectionRef = useRef({ \n    lastBeat: 0, \n    threshold: 0.35,\n    recentBeats: [] as number[]\n  });\n  const audioContextRef = useRef<AudioContext>();\n  const [permissionGranted, setPermissionGranted] = useState<boolean | null>(null);\n\n  // Initialize audio context and analyzer\n  useEffect(() => {\n    let cleanup: (() => void) | null = null;\n    \n    const initAudio = async () => {\n      try {\n        // Request microphone access\n        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n        \n        // Create audio context (cross-browser support)\n        const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext;\n        const audioContext = new AudioContextClass();\n        audioContextRef.current = audioContext;\n        \n        // Setup analyzer\n        const analyser = audioContext.createAnalyser();\n        const source = audioContext.createMediaStreamSource(stream);\n        \n        analyser.fftSize = 256; // 128 frequency bins\n        analyser.smoothingTimeConstant = 0.8;\n        source.connect(analyser);\n        analyserRef.current = analyser;\n        \n        setPermissionGranted(true);\n        \n        // Real-time audio analysis loop\n        const updateAudioData = () => {\n          if (!analyserRef.current) return;\n          \n          const dataArray = new Uint8Array(128);\n          analyserRef.current.getByteFrequencyData(dataArray);\n          \n          // Frequency band analysis (authentic mapping from research)\n          const bassRange = Array.from(dataArray.slice(0, 4));\n          const midsRange = Array.from(dataArray.slice(4, 16));\n          const trebleRange = Array.from(dataArray.slice(16, 64));\n          \n          const bass = bassRange.reduce((a, b) => a + b) / (4 * 255);\n          const mids = midsRange.reduce((a, b) => a + b) / (12 * 255);\n          const treble = trebleRange.reduce((a, b) => a + b) / (48 * 255);\n          const volume = Array.from(dataArray).reduce((a, b) => a + b) / (128 * 255);\n          \n          // Enhanced beat detection with history\n          const now = Date.now();\n          const beatDetected = bass > beatDetectionRef.current.threshold && \n                              (now - beatDetectionRef.current.lastBeat) > 250;\n          \n          if (beatDetected) {\n            beatDetectionRef.current.lastBeat = now;\n            beatDetectionRef.current.recentBeats.push(now);\n            \n            // Keep only recent beats (last 5 seconds)\n            beatDetectionRef.current.recentBeats = beatDetectionRef.current.recentBeats\n              .filter(time => now - time < 5000);\n          }\n          \n          const newAudioData: AudioAnalysisData = {\n            bass,\n            mids,\n            treble,\n            volume,\n            beatDetected,\n            spectralData: new Float32Array(dataArray)\n          };\n          \n          setAudioData(newAudioData);\n          setPhysicsParams(mapAudioToPhysics(newAudioData));\n          \n          requestAnimationFrame(updateAudioData);\n        };\n        \n        updateAudioData();\n        \n        // Cleanup function\n        cleanup = () => {\n          stream.getTracks().forEach(track => track.stop());\n          if (audioContext.state !== 'closed') {\n            audioContext.close();\n          }\n        };\n        \n      } catch (error) {\n        console.warn('Microphone access denied or failed:', error);\n        setPermissionGranted(false);\n        \n        // Fallback: simulated audio data for visual consistency\n        const simulateAudio = () => {\n          const time = Date.now() * 0.001;\n          const simulatedData: AudioAnalysisData = {\n            bass: 0.3 + Math.sin(time * 0.5) * 0.2,\n            mids: 0.3 + Math.sin(time * 0.7) * 0.2,\n            treble: 0.3 + Math.sin(time * 1.2) * 0.2,\n            volume: 0.5 + Math.sin(time * 0.3) * 0.1,\n            beatDetected: Math.sin(time * 2) > 0.8,\n            spectralData: new Float32Array(128)\n          };\n          \n          setAudioData(simulatedData);\n          setPhysicsParams(mapAudioToPhysics(simulatedData));\n          \n          setTimeout(simulateAudio, 50); // ~20fps simulation\n        };\n        \n        simulateAudio();\n      }\n    };\n    \n    initAudio();\n    \n    return cleanup || (() => {});\n  }, []);\n  \n  // Audio context state management\n  const resumeAudioContext = useCallback(async () => {\n    if (audioContextRef.current && audioContextRef.current.state === 'suspended') {\n      try {\n        await audioContextRef.current.resume();\n        console.log('Audio context resumed');\n      } catch (error) {\n        console.warn('Failed to resume audio context:', error);\n      }\n    }\n  }, []);\n  \n  // Tempo detection (bonus feature for future use)\n  const getEstimatedTempo = useCallback(() => {\n    const recentBeats = beatDetectionRef.current.recentBeats;\n    if (recentBeats.length < 4) return null;\n    \n    // Calculate intervals between recent beats\n    const intervals = [];\n    for (let i = 1; i < recentBeats.length; i++) {\n      intervals.push(recentBeats[i] - recentBeats[i - 1]);\n    }\n    \n    // Average interval to BPM\n    const avgInterval = intervals.reduce((a, b) => a + b) / intervals.length;\n    return Math.round(60000 / avgInterval); // BPM\n  }, []);\n  \n  return {\n    audioData,\n    physicsParams,\n    permissionGranted,\n    resumeAudioContext,\n    getEstimatedTempo\n  };\n}\n\n// Utility hook for components that only need physics parameters\nexport function useAudioPhysics() {\n  const { physicsParams } = useAudioReactiveEngine();\n  return physicsParams;\n}\n\n// Export the mapping function for consistency across components\nexport { mapAudioToPhysics };\nexport type { AudioAnalysisData, AudioReactiveParams };\n"