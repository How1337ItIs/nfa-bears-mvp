'use client';\n\nimport { useEffect, useState, useRef, useCallback } from 'react';\nimport { AUDIO_PHYSICS_MAPPING } from '@/lib/fluid/config';\n\n// CENTRALIZED AUDIO REACTIVITY - Single analyzer for entire app\n// Prevents resource conflicts from multiple analyzers per route\n\nexport interface AudioAnalysisData {\n  bass: number;                    // 0-1, low frequency energy (0-4 bins)\n  mids: number;                    // 0-1, mid frequency energy (4-16 bins)\n  treble: number;                  // 0-1, high frequency energy (16-64 bins)\n  volume: number;                  // 0-1, overall volume level\n  beatDetected: boolean;           // Beat detection flag\n  spectralData: Float32Array;      // Raw frequency data for advanced use\n  tempo?: number;                  // Estimated BPM (optional)\n}\n\nexport interface AudioReactiveParams {\n  splatForce: number;              // Bass -> splat intensity (8-23 range)\n  thermalRate: number;             // Mids -> convection frequency (2-8 range)\n  colorPhase: number;              // Treble -> thin-film color shift (0-2Ï€)\n  globalIntensity: number;         // Volume -> overall effect strength (0.4-1.0)\n}\n\ninterface BeatDetection {\n  threshold: number;\n  lastBeat: number;\n  recentBeats: number[];\n  sensitivity: number;\n}\n\n// UNIFIED AUDIO-TO-PHYSICS MAPPING\n// Single source of truth prevents conflicting interpretations\nfunction mapAudioToPhysics(audio: AudioAnalysisData): AudioReactiveParams {\n  const { SPLAT_FORCE, THERMAL_RATE, COLOR_PHASE, GLOBAL_INTENSITY } = AUDIO_PHYSICS_MAPPING;\n  \n  return {\n    splatForce: SPLAT_FORCE.BASE + audio.bass * (SPLAT_FORCE.MAX - SPLAT_FORCE.MIN),\n    thermalRate: THERMAL_RATE.BASE + audio.mids * (THERMAL_RATE.MAX - THERMAL_RATE.MIN),\n    colorPhase: COLOR_PHASE.BASE + audio.treble * COLOR_PHASE.MAX,\n    globalIntensity: GLOBAL_INTENSITY.BASE + audio.volume * (GLOBAL_INTENSITY.MAX - GLOBAL_INTENSITY.BASE)\n  };\n}\n\n// BEAT DETECTION ALGORITHM\nfunction detectBeat(\n  currentBass: number,\n  detection: BeatDetection\n): { beatDetected: boolean; newThreshold: number } {\n  const now = Date.now();\n  const timeSinceLastBeat = now - detection.lastBeat;\n  \n  // Adaptive threshold based on recent activity\n  const avgBass = detection.recentBeats.length > 0 \n    ? detection.recentBeats.reduce((a, b) => a + b) / detection.recentBeats.length\n    : 0.3;\n  \n  const adaptiveThreshold = Math.max(0.2, avgBass * 1.2);\n  \n  // Beat detected if bass exceeds threshold and enough time has passed\n  const beatDetected = currentBass > adaptiveThreshold && \n                      timeSinceLastBeat > 250; // Minimum 250ms between beats\n  \n  return {\n    beatDetected,\n    newThreshold: adaptiveThreshold\n  };\n}\n\n// TEMPO ESTIMATION\nfunction estimateTempo(recentBeats: number[]): number | undefined {\n  if (recentBeats.length < 4) return undefined;\n  \n  // Calculate intervals between beats\n  const intervals: number[] = [];\n  for (let i = 1; i < recentBeats.length; i++) {\n    intervals.push(recentBeats[i] - recentBeats[i - 1]);\n  }\n  \n  // Remove outliers (beats that are too far apart or too close)\n  const validIntervals = intervals.filter(interval => \n    interval > 300 && interval < 2000 // 30-200 BPM range\n  );\n  \n  if (validIntervals.length < 2) return undefined;\n  \n  // Average interval to BPM\n  const avgInterval = validIntervals.reduce((a, b) => a + b) / validIntervals.length;\n  return Math.round(60000 / avgInterval);\n}\n\n// MAIN AUDIO REACTIVE HOOK\nexport function useAudioReactive() {\n  const [audioData, setAudioData] = useState<AudioAnalysisData>({\n    bass: 0.3,\n    mids: 0.3,\n    treble: 0.3,\n    volume: 0.5,\n    beatDetected: false,\n    spectralData: new Float32Array(128)\n  });\n  \n  const [physicsParams, setPhysicsParams] = useState<AudioReactiveParams>(\n    mapAudioToPhysics(audioData)\n  );\n  \n  const [microphonePermission, setMicrophonePermission] = useState<'granted' | 'denied' | 'pending'>('pending');\n  \n  // Refs for audio processing\n  const analyserRef = useRef<AnalyserNode>();\n  const audioContextRef = useRef<AudioContext>();\n  const streamRef = useRef<MediaStream>();\n  const animationRef = useRef<number>();\n  \n  // Beat detection state\n  const beatDetectionRef = useRef<BeatDetection>({\n    threshold: 0.35,\n    lastBeat: 0,\n    recentBeats: [],\n    sensitivity: 1.0\n  });\n  \n  // Audio context initialization\n  const initializeAudioContext = useCallback(async () => {\n    try {\n      // Request microphone access\n      const stream = await navigator.mediaDevices.getUserMedia({ \n        audio: {\n          echoCancellation: false,\n          noiseSuppression: false,\n          autoGainControl: false\n        }\n      });\n      \n      streamRef.current = stream;\n      setMicrophonePermission('granted');\n      \n      // Create audio context with cross-browser support\n      const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext;\n      const audioContext = new AudioContextClass();\n      audioContextRef.current = audioContext;\n      \n      // Setup analyzer node\n      const analyser = audioContext.createAnalyser();\n      const source = audioContext.createMediaStreamSource(stream);\n      \n      analyser.fftSize = 256;                 // 128 frequency bins\n      analyser.smoothingTimeConstant = 0.8;   // Temporal smoothing\n      analyser.minDecibels = -90;\n      analyser.maxDecibels = -10;\n      \n      source.connect(analyser);\n      analyserRef.current = analyser;\n      \n      console.log('Audio context initialized successfully');\n      \n    } catch (error) {\n      console.warn('Microphone access denied or failed:', error);\n      setMicrophonePermission('denied');\n      \n      // Start simulated audio for consistent visual behavior\n      startSimulatedAudio();\n    }\n  }, []);\n  \n  // Simulated audio for when microphone is unavailable\n  const startSimulatedAudio = useCallback(() => {\n    console.log('Starting simulated audio reactivity');\n    \n    const simulateAudio = () => {\n      const time = Date.now() * 0.001;\n      \n      // Generate realistic-looking audio data\n      const simulatedData: AudioAnalysisData = {\n        bass: 0.25 + Math.sin(time * 0.5) * 0.15 + Math.random() * 0.1,\n        mids: 0.3 + Math.sin(time * 0.7) * 0.2 + Math.random() * 0.1,\n        treble: 0.35 + Math.sin(time * 1.2) * 0.25 + Math.random() * 0.1,\n        volume: 0.4 + Math.sin(time * 0.3) * 0.2 + Math.random() * 0.05,\n        beatDetected: Math.sin(time * 2.1) > 0.7 && Math.random() > 0.8,\n        spectralData: new Float32Array(128).map(() => Math.random() * 128),\n        tempo: 120 + Math.sin(time * 0.1) * 20 // 100-140 BPM range\n      };\n      \n      setAudioData(simulatedData);\n      setPhysicsParams(mapAudioToPhysics(simulatedData));\n      \n      // Continue simulation\n      setTimeout(simulateAudio, 50); // ~20fps\n    };\n    \n    simulateAudio();\n  }, []);\n  \n  // Real-time audio analysis loop\n  const startAudioAnalysis = useCallback(() => {\n    if (!analyserRef.current) return;\n    \n    const analyzeAudio = () => {\n      if (!analyserRef.current) return;\n      \n      const dataArray = new Uint8Array(128);\n      analyserRef.current.getByteFrequencyData(dataArray);\n      \n      // Frequency band analysis (research-validated ranges)\n      const bassRange = Array.from(dataArray.slice(0, 4));      // 0-4 bins\n      const midsRange = Array.from(dataArray.slice(4, 16));     // 4-16 bins\n      const trebleRange = Array.from(dataArray.slice(16, 64));  // 16-64 bins\n      const fullRange = Array.from(dataArray);\n      \n      // Calculate normalized values (0-1 range)\n      const bass = bassRange.reduce((a, b) => a + b) / (4 * 255);\n      const mids = midsRange.reduce((a, b) => a + b) / (12 * 255);\n      const treble = trebleRange.reduce((a, b) => a + b) / (48 * 255);\n      const volume = fullRange.reduce((a, b) => a + b) / (128 * 255);\n      \n      // Beat detection with adaptive threshold\n      const { beatDetected, newThreshold } = detectBeat(bass, beatDetectionRef.current);\n      beatDetectionRef.current.threshold = newThreshold;\n      \n      if (beatDetected) {\n        const now = Date.now();\n        beatDetectionRef.current.lastBeat = now;\n        beatDetectionRef.current.recentBeats.push(bass);\n        \n        // Keep only recent beats (last 10 seconds)\n        beatDetectionRef.current.recentBeats = beatDetectionRef.current.recentBeats\n          .slice(-20); // Keep last 20 beats max\n      }\n      \n      // Tempo estimation\n      const recentBeatTimes = beatDetectionRef.current.recentBeats.map((_, i) => \n        beatDetectionRef.current.lastBeat - (beatDetectionRef.current.recentBeats.length - 1 - i) * 500\n      );\n      const tempo = estimateTempo(recentBeatTimes);\n      \n      const newAudioData: AudioAnalysisData = {\n        bass,\n        mids,\n        treble,\n        volume,\n        beatDetected,\n        spectralData: new Float32Array(dataArray),\n        tempo\n      };\n      \n      setAudioData(newAudioData);\n      setPhysicsParams(mapAudioToPhysics(newAudioData));\n      \n      // Continue analysis loop\n      animationRef.current = requestAnimationFrame(analyzeAudio);\n    };\n    \n    analyzeAudio();\n  }, []);\n  \n  // Initialize audio system on mount\n  useEffect(() => {\n    initializeAudioContext();\n    \n    // Cleanup function\n    return () => {\n      if (animationRef.current) {\n        cancelAnimationFrame(animationRef.current);\n      }\n      \n      if (streamRef.current) {\n        streamRef.current.getTracks().forEach(track => track.stop());\n      }\n      \n      if (audioContextRef.current && audioContextRef.current.state !== 'closed') {\n        audioContextRef.current.close();\n      }\n    };\n  }, [initializeAudioContext]);\n  \n  // Start analysis when analyzer is ready\n  useEffect(() => {\n    if (analyserRef.current && microphonePermission === 'granted') {\n      startAudioAnalysis();\n    }\n  }, [startAudioAnalysis, microphonePermission]);\n  \n  // Audio context state management (handle browser autoplay policies)\n  const resumeAudioContext = useCallback(async () => {\n    if (audioContextRef.current && audioContextRef.current.state === 'suspended') {\n      try {\n        await audioContextRef.current.resume();\n        console.log('Audio context resumed');\n      } catch (error) {\n        console.warn('Failed to resume audio context:', error);\n      }\n    }\n  }, []);\n  \n  // Manual sensitivity adjustment\n  const adjustBeatSensitivity = useCallback((sensitivity: number) => {\n    beatDetectionRef.current.sensitivity = Math.max(0.1, Math.min(2.0, sensitivity));\n  }, []);\n  \n  return {\n    audioData,\n    physicsParams,\n    microphonePermission,\n    resumeAudioContext,\n    adjustBeatSensitivity\n  };\n}\n\n// UTILITY HOOKS\n\n// Hook for components that only need physics parameters\nexport function useAudioPhysics(): AudioReactiveParams {\n  const { physicsParams } = useAudioReactive();\n  return physicsParams;\n}\n\n// Hook for components that only need beat detection\nexport function useBeatDetection(): { beatDetected: boolean; tempo?: number } {\n  const { audioData } = useAudioReactive();\n  return {\n    beatDetected: audioData.beatDetected,\n    tempo: audioData.tempo\n  };\n}\n\n// Export mapping function and types for consistency\nexport { mapAudioToPhysics };\nexport type { AudioAnalysisData, AudioReactiveParams };\n"